{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "024d44fa",
   "metadata": {},
   "source": [
    "# 3 Activation Functions and Vanishing Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8ca7b7",
   "metadata": {},
   "source": [
    "The purpose of this task is to get familiar with different activation functions commonly used in artificial neural networks, as well as with some particular problems associated with some of the functions. We will mostly use the common libraries that you have learned to work with in previous assignments for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a3f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60846af5",
   "metadata": {},
   "source": [
    "## 3.1 Activation Functions\n",
    "Your first task will be to give a brief overview of the most popular activation functions used in machine learning, both in terms of mathematical definition and plotting. \n",
    "\n",
    "_As usual, for all plots you will produce, keep in mind to add title, proper axis ticks, and axis labels to the plots and make sure they're readable and comprehensible. Whenever you have to plot two different lines, please use different colors for them and add a legend to the plot._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d3006",
   "metadata": {},
   "source": [
    "### Task 3.1.1 Logistic function\n",
    "The logistic function is one particular Sigmoid activation function. Given some neuron input $x$, it is defined by the formula $$f(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "1.) Implement a python function that calculates the logistic function, then\n",
    "\n",
    "2.) Implement the derivative of the logistic function (either you look it up online or you try to derive the equation yourself), and finally\n",
    "\n",
    "3.) Write a function that plots both the logistic function $f(x)$ as well as its derivative/gradient $\\frac{\\partial f(x)}{\\partial x}$ for the some given interval of input values, denoted by the lower bound $x_{\\text{min}}$ and the upper bound $x_{\\text{max}}$. As an example, given the function call ```plot_logistic(-8, 8)```, we want to plot the function and its gradient for the interval $x \\in \\{-8, 8\\}$. Use the third code box below to test your plots against different combinations of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ac6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    # your code here\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_grad(x):\n",
    "    # your code here\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeda58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logistic(x_min, x_max):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7b30be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function call to test your code for different interval boundaries.\n",
    "plot_logistic(-8, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7062d1c",
   "metadata": {},
   "source": [
    "### Task 3.1.2 Hyperbolic tangent\n",
    "\n",
    "The hyperbolic tangent $(tanh)$ is another type of Sigmoid activation function. Given some neuron input $x$, it is defined by the formula $$f(x) = tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "1.) Implement a python function that calculates the hyperbolic tangent, then\n",
    "\n",
    "2.) Implement the derivative of the hyperbolic tangent (either you look it up online or you try to derive the equation yourself), and finally\n",
    "\n",
    "3.) Write a function that plots both the hyperbolic tangent $f(x)$ as well as its derivative/gradient $\\frac{\\partial f(x)}{\\partial x}$ for the some given interval of input values, denoted by the lower bound $x_{\\text{min}}$ and the upper bound $x_{\\text{max}}$. As an example, given the function call ```plot_tanh(-8, 8)```, we want to plot the function and its gradient for the interval $x \\in \\{-8, 8\\}$. Use the third code box below to test your plots against different combinations of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72096982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    # your code here\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3619e18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_grad(x):\n",
    "    # your code here\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9a8502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tanh(x_min, x_max):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23d1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function call to test your code for different interval boundaries.\n",
    "plot_tanh(-8, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee63463",
   "metadata": {},
   "source": [
    "### Task 3.1.3 Rectified Linear Unit\n",
    "The Rectified Linear Unit (ReLU) is another well-known actiavtion function used in neural networks. Given some neuron input $x$, it is defined by the formula \n",
    "\n",
    "$$f(x) = \\begin{cases}\n",
    "      x, & \\text{if} \\; x>0 \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}$$\n",
    "\n",
    "1.) Implement a python function that calculates the ReLU function, then\n",
    "\n",
    "2.) implement the derivative of the ReLU function (either you look it up online or you try to derive the equation yourself), and finally\n",
    "\n",
    "3.) write a function that plots both the ReLU function $f(x)$ as well as its derivative/gradient $\\frac{\\partial f(x)}{\\partial x}$ for the some given interval of input values, denoted by the lower bound $x_{\\text{min}}$ and the upper bound $x_{\\text{max}}$. As an example, given the function call ```plot_relu(-8, 8)```, we want to plot the function and its gradient for the interval $x \\in \\{-8, 8\\}$. Use the third code box below to test your plots against different combinations of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc47c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    # your code here\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ce4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(x):\n",
    "    # your code here\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c89d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relu(x_min, x_max):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c057cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function call to test your code for different interval boundaries.\n",
    "plot_relu(-8, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b379be05",
   "metadata": {},
   "source": [
    "## Task 3.2 Vanishing Gradients\n",
    "\n",
    "Explain the problem of **vanishing gradients** in your own words by taking a look at the plots you just generated above and answering the questions below:\n",
    "\n",
    "- When can vanishing gradients occur?\n",
    "- Assuming our input values are positive, which activation functions are prone to vanishing gradients, and why?\n",
    "- Which layers in a feed-forward neural network are most affected by vanishing gradients?\n",
    "- Considering the causes of vanishing gradients, what could instead cause the **exploding gradients** problem in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd1d26e",
   "metadata": {},
   "source": [
    "**ANSWER:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc00503d",
   "metadata": {},
   "source": [
    ">_your answer here_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
